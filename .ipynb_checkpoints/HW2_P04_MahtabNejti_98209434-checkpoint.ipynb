{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahtab Nejati 98209434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTICE:\n",
    "## Rename the dataset file to \"q4.csv\" before execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "MAX_MEMORY = \"8g\"\n",
    "spark = SparkSession.builder.appName('App Name').master(\"local[*]\").config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY).getOrCreate()\n",
    "sqlc = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /home/mahtab/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mahtab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mahtab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mahtab/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download(\"omw\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "## Workaround for the issue I've talked about with Mr. Rahimian\n",
    "while(True):\n",
    "    try:\n",
    "        sc.broadcast(wordnet)\n",
    "        break\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from itertools import combinations\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlc.read.option('header','true').csv('q4.csv')\n",
    "df = df.drop(*['id'])\n",
    "df = df.toDF('id','text','emotion')\n",
    "emotions = df.select('emotion').dropDuplicates().collect()\n",
    "emotions = sorted([emotions[i]['emotion'] for i in range(len(emotions))])\n",
    "dataset_size = df.count()\n",
    "\n",
    "# df.head(10)\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Preprocessing the text\n",
    "### Since stemming totally distorted the words and caused numerous spelling errors and POS distortion, I'v chosen not to apply it and only use lemmatization with appropriate word POS.\n",
    "### You can un-comment the stemming to see the results with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getWordnetPOS(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def uniqueList(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "def preprocessText(row):\n",
    "    text = row['text']\n",
    "    \n",
    "    # LowerCase\n",
    "    sent = text.lower()\n",
    "    \n",
    "    # Remove all digits\n",
    "    sent = ''.join([i for i in sent if not i.isdigit()])\n",
    "    \n",
    "    # Get list of words\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        \n",
    "        # Remove all words with length less than 2\n",
    "        if not len(word) > 1:\n",
    "            continue\n",
    "        \n",
    "        # Remove all punctuations\n",
    "        if not word.isalnum():\n",
    "            continue\n",
    "        \n",
    "        # Lemmatizing the word\n",
    "        new_word = lemmatizer.lemmatize(word,getWordnetPOS(word))\n",
    "        \n",
    "        # Stemming the word\n",
    "#         new_word = stemmer.stem(new_word)\n",
    "        \n",
    "        # Remove all stop words\n",
    "        if new_word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # Remove all words with length less than 2 agian\n",
    "        if not len(word) > 1:\n",
    "            continue\n",
    "        \n",
    "        result.append(new_word)\n",
    "    \n",
    "    result = uniqueList(result)\n",
    "        \n",
    "    return row['id'],result,row['emotion'],len(result)\n",
    "\n",
    "\n",
    "\n",
    "rdd = df.rdd.map(lambda row: (preprocessText(row)))\n",
    "\n",
    "# rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Saving distinct words to a json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTICE:\n",
    "### Runing the cell below will take quite some time to run.\n",
    "### I have pickled the results into the file 'HW2_P04_MahtabNejti_98209434_Pickled_Preproc' for further use. \n",
    "### Skip the following cell and execute the next one to load data in or uncomment the code in the cell and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baskets = rdd.collect()\n",
    "# with open('HW2_P04_MahtabNejti_98209434_Pickled_Preproc','wb') as f:\n",
    "#     pickle.dump(baskets,f)\n",
    "# rdd = sc.parallelize(baskets).map(lambda row: row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HW2_P04_MahtabNejti_98209434_Pickled_Preproc','rb') as f:\n",
    "    baskets = pickle.load(f)\n",
    "rdd = sc.parallelize(baskets).map(lambda row: row[1:])\n",
    "# rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = rdd.map(lambda row: row[-1]).max()\n",
    "# max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = rdd.flatMap(lambda row: row[0])\n",
    "\n",
    "# distinct_words_df = words.map(lambda row: (row,)).toDF(['word']).dropDuplicates()\n",
    "# distinct_words_df = distinct_words_df.withColumn(\"id\", 1+monotonically_increasing_id())\n",
    "# windowSpec = Window.orderBy(\"id\")\n",
    "# distinct_words_df = distinct_words_df.withColumn(\"id\", row_number().over(windowSpec))\n",
    "# json_rdd = distinct_words_df.rdd.map(lambda row: {row[0]:row[1]})\n",
    "# word_dict = json_rdd.reduce(lambda a,b: {**a,**b})\n",
    "# with open('HW2_P04_MahtabNejti_98209434_ByWords.json', 'w') as f:\n",
    "#     json.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I also choose to store the distinct words as a reverse dictionary for furthur use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_dict = {v:k for k,v in word_dict.items()}\n",
    "# with open('HW2_P04_MahtabNejti_98209434_ByIDs.json', 'w') as f:\n",
    "#     json.dump(id_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: How big of a data can I load into my system's memory\n",
    "### Answers in pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: SON Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toCount(row,emotion_list):\n",
    "    counts = np.zeros(len(emotion_list)+1,dtype=int)\n",
    "    counts[-1] = 1\n",
    "    counts[emotions.index(row[1])] = 1\n",
    "    newRow = ([tuple([x]) for x in row[0]],counts)\n",
    "    return newRow\n",
    "\n",
    "countable = rdd.map(lambda row: toCount(row,emotions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCandidateItemsets(part,size,prevFreq):\n",
    "    new_baskets = []\n",
    "    for basket in part:\n",
    "        candidates = []\n",
    "        basket_list = [x[0] for x in basket[0]]\n",
    "        itemset_list = [tuple(sorted(x)) for x in combinations(basket_list,size)]\n",
    "        for itemset in itemset_list:\n",
    "            if size == 2:\n",
    "                subsets = [(x,) for x in itemset]\n",
    "            else:\n",
    "                subsets = [tuple(sorted(x)) for x in combinations(itemset,size-1)]\n",
    "            if all(x in prevFreq for x in subsets):\n",
    "                candidates.append(itemset)\n",
    "        new_baskets.append((candidates,basket[1].copy()))\n",
    "    return new_baskets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequentItemsets(partition,size,s,prevFreq=[]):\n",
    "    part = list(partition)\n",
    "    support = len(part)*s\n",
    "    counts = {}\n",
    "    if size > 1:\n",
    "        part = getCandidateItemsets(part,size,prevFreq)\n",
    "    for basket in part:\n",
    "        for itemset in basket[0]:\n",
    "            try:\n",
    "                counts[itemset] += basket[1].copy()\n",
    "            except KeyError:\n",
    "                counts[itemset] = basket[1].copy()\n",
    "    freqItemset = {key:value for key,value in counts.items() if value[-1] >= support}\n",
    "    return (freqItemset,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure results are actually frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSupportedItemset(row,size,supported):\n",
    "    allSupported = []\n",
    "    basket = [x[0] for x in row[0]]\n",
    "    if size == 1:\n",
    "        itemset_list = [x for x in row[0]]\n",
    "    else:\n",
    "        itemset_list = [tuple(sorted(x)) for x in combinations(basket,size)]\n",
    "    for itemset in itemset_list:\n",
    "        if itemset in supported:\n",
    "            allSupported.append((itemset,row[1].copy()))\n",
    "    return allSupported\n",
    "    \n",
    "\n",
    "def countSupportedItemsets(rdd,size,supported):\n",
    "    data = rdd.flatMap(\n",
    "        lambda row: getSupportedItemset(row,size,supported)).reduceByKey(\n",
    "        lambda a,b : np.array(a.copy()+b.copy(),dtype=int))\n",
    "    return dict(data.collect())\n",
    "    return data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def son(rdd,s,dataset_size,max_size):\n",
    "    support = s*dataset_size\n",
    "    freqItemsets = {}\n",
    "    for i in range(1,max_size+1):\n",
    "        try:\n",
    "            prevFreq = list(freqItemsets['size_'+str(i-1)].keys())\n",
    "        except KeyError:\n",
    "            prevFreq = []\n",
    "        allSupportedFrequents = rdd.mapPartitions(lambda part: getFrequentItemsets(part,i,s*0.9,prevFreq)).collect()\n",
    "        supportedFrequents = reduce(\n",
    "            lambda a,b : dict.fromkeys(set(a.keys()).union(set(b.keys())),0),allSupportedFrequents)\n",
    "        supportedFrequents = countSupportedItemsets(rdd,i,list(supportedFrequents.keys()))\n",
    "        freqItemsets['size_'+str(i)] = { k:v for k,v in supportedFrequents.items() if v[-1] >= support}\n",
    "    return freqItemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = son(countable,0.05,dataset_size,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>itemset</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>(get,)</td>\n",
       "      <td>3759</td>\n",
       "      <td>2494</td>\n",
       "      <td>7482</td>\n",
       "      <td>1608</td>\n",
       "      <td>6004</td>\n",
       "      <td>768</td>\n",
       "      <td>22115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(feel,)</td>\n",
       "      <td>39099</td>\n",
       "      <td>33312</td>\n",
       "      <td>99177</td>\n",
       "      <td>24073</td>\n",
       "      <td>85257</td>\n",
       "      <td>10616</td>\n",
       "      <td>291534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>(im,)</td>\n",
       "      <td>5912</td>\n",
       "      <td>4784</td>\n",
       "      <td>12928</td>\n",
       "      <td>2963</td>\n",
       "      <td>11228</td>\n",
       "      <td>1232</td>\n",
       "      <td>39047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>(really,)</td>\n",
       "      <td>2475</td>\n",
       "      <td>1870</td>\n",
       "      <td>5738</td>\n",
       "      <td>1491</td>\n",
       "      <td>5012</td>\n",
       "      <td>661</td>\n",
       "      <td>17247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>(like,)</td>\n",
       "      <td>6998</td>\n",
       "      <td>4268</td>\n",
       "      <td>16984</td>\n",
       "      <td>5715</td>\n",
       "      <td>14888</td>\n",
       "      <td>1614</td>\n",
       "      <td>50467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>(go,)</td>\n",
       "      <td>2631</td>\n",
       "      <td>2636</td>\n",
       "      <td>6243</td>\n",
       "      <td>1381</td>\n",
       "      <td>5576</td>\n",
       "      <td>695</td>\n",
       "      <td>19162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>(time,)</td>\n",
       "      <td>2425</td>\n",
       "      <td>2014</td>\n",
       "      <td>5606</td>\n",
       "      <td>1372</td>\n",
       "      <td>5116</td>\n",
       "      <td>682</td>\n",
       "      <td>17215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>(make,)</td>\n",
       "      <td>2287</td>\n",
       "      <td>2177</td>\n",
       "      <td>7831</td>\n",
       "      <td>1639</td>\n",
       "      <td>5805</td>\n",
       "      <td>576</td>\n",
       "      <td>20315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>(know,)</td>\n",
       "      <td>2502</td>\n",
       "      <td>2317</td>\n",
       "      <td>5514</td>\n",
       "      <td>1552</td>\n",
       "      <td>5452</td>\n",
       "      <td>700</td>\n",
       "      <td>18037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>(feel, make)</td>\n",
       "      <td>2175</td>\n",
       "      <td>2144</td>\n",
       "      <td>7720</td>\n",
       "      <td>1614</td>\n",
       "      <td>5714</td>\n",
       "      <td>570</td>\n",
       "      <td>19937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>(feel, know)</td>\n",
       "      <td>2398</td>\n",
       "      <td>2273</td>\n",
       "      <td>5372</td>\n",
       "      <td>1495</td>\n",
       "      <td>5328</td>\n",
       "      <td>689</td>\n",
       "      <td>17555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>(feel, time)</td>\n",
       "      <td>2325</td>\n",
       "      <td>1953</td>\n",
       "      <td>5469</td>\n",
       "      <td>1322</td>\n",
       "      <td>5008</td>\n",
       "      <td>672</td>\n",
       "      <td>16749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>(feel, im)</td>\n",
       "      <td>5806</td>\n",
       "      <td>4733</td>\n",
       "      <td>12755</td>\n",
       "      <td>2887</td>\n",
       "      <td>11088</td>\n",
       "      <td>1215</td>\n",
       "      <td>38484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>(feel, really)</td>\n",
       "      <td>2401</td>\n",
       "      <td>1847</td>\n",
       "      <td>5651</td>\n",
       "      <td>1450</td>\n",
       "      <td>4932</td>\n",
       "      <td>646</td>\n",
       "      <td>16927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>(feel, like)</td>\n",
       "      <td>6896</td>\n",
       "      <td>4241</td>\n",
       "      <td>16852</td>\n",
       "      <td>5608</td>\n",
       "      <td>14779</td>\n",
       "      <td>1608</td>\n",
       "      <td>49984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>(feel, go)</td>\n",
       "      <td>2485</td>\n",
       "      <td>2516</td>\n",
       "      <td>6104</td>\n",
       "      <td>1337</td>\n",
       "      <td>5411</td>\n",
       "      <td>682</td>\n",
       "      <td>18535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>(feel, get)</td>\n",
       "      <td>3564</td>\n",
       "      <td>2408</td>\n",
       "      <td>7267</td>\n",
       "      <td>1557</td>\n",
       "      <td>5799</td>\n",
       "      <td>754</td>\n",
       "      <td>21349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    size         itemset  anger   fear    joy   love  sadness  surprise  \\\n",
       "0      1          (get,)   3759   2494   7482   1608     6004       768   \n",
       "1      1         (feel,)  39099  33312  99177  24073    85257     10616   \n",
       "2      1           (im,)   5912   4784  12928   2963    11228      1232   \n",
       "3      1       (really,)   2475   1870   5738   1491     5012       661   \n",
       "4      1         (like,)   6998   4268  16984   5715    14888      1614   \n",
       "5      1           (go,)   2631   2636   6243   1381     5576       695   \n",
       "6      1         (time,)   2425   2014   5606   1372     5116       682   \n",
       "7      1         (make,)   2287   2177   7831   1639     5805       576   \n",
       "8      1         (know,)   2502   2317   5514   1552     5452       700   \n",
       "9      2    (feel, make)   2175   2144   7720   1614     5714       570   \n",
       "10     2    (feel, know)   2398   2273   5372   1495     5328       689   \n",
       "11     2    (feel, time)   2325   1953   5469   1322     5008       672   \n",
       "12     2      (feel, im)   5806   4733  12755   2887    11088      1215   \n",
       "13     2  (feel, really)   2401   1847   5651   1450     4932       646   \n",
       "14     2    (feel, like)   6896   4241  16852   5608    14779      1608   \n",
       "15     2      (feel, go)   2485   2516   6104   1337     5411       682   \n",
       "16     2     (feel, get)   3564   2408   7267   1557     5799       754   \n",
       "\n",
       "     total  \n",
       "0    22115  \n",
       "1   291534  \n",
       "2    39047  \n",
       "3    17247  \n",
       "4    50467  \n",
       "5    19162  \n",
       "6    17215  \n",
       "7    20315  \n",
       "8    18037  \n",
       "9    19937  \n",
       "10   17555  \n",
       "11   16749  \n",
       "12   38484  \n",
       "13   16927  \n",
       "14   49984  \n",
       "15   18535  \n",
       "16   21349  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for k,v in results['size_1'].items():\n",
    "    entry = {}\n",
    "    entry['size'] = 1\n",
    "    entry['itemset'] = k\n",
    "    entry['anger'] = v[0]\n",
    "    entry['fear'] = v[1]\n",
    "    entry['joy'] = v[2]\n",
    "    entry['love'] = v[3]\n",
    "    entry['sadness'] = v[4]\n",
    "    entry['surprise'] = v[5]\n",
    "    entry['total'] = v[6]\n",
    "    data.append(entry)\n",
    "for k,v in results['size_2'].items():\n",
    "    entry = {}\n",
    "    entry['size'] = 2\n",
    "    entry['itemset'] = k\n",
    "    entry['anger'] = v[0]\n",
    "    entry['fear'] = v[1]\n",
    "    entry['joy'] = v[2]\n",
    "    entry['love'] = v[3]\n",
    "    entry['sadness'] = v[4]\n",
    "    entry['surprise'] = v[5]\n",
    "    entry['total'] = v[6]\n",
    "    data.append(entry)\n",
    "freqItemsets = pd.DataFrame(data)\n",
    "freqItemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E: How to compute the probability P(feeling|w1 w2 ... wn)\n",
    "## Answers in pdf file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F: Calculate the emotion of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': 87362,\n",
       " 'love': 24775,\n",
       " 'anger': 41315,\n",
       " 'fear': 34381,\n",
       " 'joy': 101419,\n",
       " 'surprise': 10748}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_counted = dict(rdd.map(lambda row : (row[1],1)).reduceByKey(lambda a,b: a+b).collect())\n",
    "words_counted = countable.flatMap(\n",
    "    lambda row: [(a[0],row[1]) for a in row[0]]).reduceByKey(\n",
    "    lambda a,b : a+b).map(\n",
    "    lambda row: {'word':row[0],\n",
    "                 emotions[0]:row[1][0],\n",
    "                 emotions[1]:row[1][1],\n",
    "                 emotions[2]:row[1][2],\n",
    "                 emotions[3]:row[1][3],\n",
    "                 emotions[4]:row[1][4],\n",
    "                 emotions[5]:row[1][5],\n",
    "                 'total':row[1][6]}).collect()\n",
    "words_df = pd.DataFrame(words_counted)\n",
    "emotions_counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel</td>\n",
       "      <td>39099</td>\n",
       "      <td>33312</td>\n",
       "      <td>99177</td>\n",
       "      <td>24073</td>\n",
       "      <td>85257</td>\n",
       "      <td>10616</td>\n",
       "      <td>291534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>job</td>\n",
       "      <td>243</td>\n",
       "      <td>226</td>\n",
       "      <td>719</td>\n",
       "      <td>124</td>\n",
       "      <td>543</td>\n",
       "      <td>55</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>position</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>180</td>\n",
       "      <td>43</td>\n",
       "      <td>119</td>\n",
       "      <td>14</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happen</td>\n",
       "      <td>323</td>\n",
       "      <td>386</td>\n",
       "      <td>684</td>\n",
       "      <td>151</td>\n",
       "      <td>762</td>\n",
       "      <td>141</td>\n",
       "      <td>2447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im</td>\n",
       "      <td>5912</td>\n",
       "      <td>4784</td>\n",
       "      <td>12928</td>\n",
       "      <td>2963</td>\n",
       "      <td>11228</td>\n",
       "      <td>1232</td>\n",
       "      <td>39047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52163</th>\n",
       "      <td>rikki</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52164</th>\n",
       "      <td>conveyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52165</th>\n",
       "      <td>bebo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52166</th>\n",
       "      <td>frugalistas</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52167</th>\n",
       "      <td>tooheys</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52168 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  anger   fear    joy   love  sadness  surprise   total\n",
       "0             feel  39099  33312  99177  24073    85257     10616  291534\n",
       "1              job    243    226    719    124      543        55    1910\n",
       "2         position     70     72    180     43      119        14     498\n",
       "3           happen    323    386    684    151      762       141    2447\n",
       "4               im   5912   4784  12928   2963    11228      1232   39047\n",
       "...            ...    ...    ...    ...    ...      ...       ...     ...\n",
       "52163        rikki      0      0      1      0        0         0       1\n",
       "52164     conveyer      0      0      1      0        0         0       1\n",
       "52165         bebo      0      0      0      0        1         0       1\n",
       "52166  frugalistas      0      0      1      0        0         0       1\n",
       "52167      tooheys      0      0      1      0        0         0       1\n",
       "\n",
       "[52168 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': 0.29120666666666667,\n",
       " 'love': 0.08258333333333333,\n",
       " 'anger': 0.13771666666666665,\n",
       " 'fear': 0.11460333333333333,\n",
       " 'joy': 0.3380633333333333,\n",
       " 'surprise': 0.035826666666666666}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_prob = words_df.copy()\n",
    "s = words_df['total'].sum()\n",
    "words_prob['total'] /= s\n",
    "emotions_prob = emotions_counted.copy()\n",
    "for em in emotions:\n",
    "    s = words_df[em].sum()\n",
    "    words_prob[em]/=s\n",
    "    emotions_prob[em]/=300000\n",
    "emotions_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel</td>\n",
       "      <td>0.107122</td>\n",
       "      <td>0.109316</td>\n",
       "      <td>0.107256</td>\n",
       "      <td>0.101133</td>\n",
       "      <td>0.112475</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>1.084063e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>job</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>7.102294e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>position</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>1.851802e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happen</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>9.099117e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im</td>\n",
       "      <td>0.016197</td>\n",
       "      <td>0.015699</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>0.012466</td>\n",
       "      <td>1.451954e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52163</th>\n",
       "      <td>rikki</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.718478e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52164</th>\n",
       "      <td>conveyer</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.718478e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52165</th>\n",
       "      <td>bebo</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.718478e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52166</th>\n",
       "      <td>frugalistas</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.718478e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52167</th>\n",
       "      <td>tooheys</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.718478e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52168 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word     anger      fear       joy      love   sadness  \\\n",
       "0             feel  0.107122  0.109316  0.107256  0.101133  0.112475   \n",
       "1              job  0.000666  0.000742  0.000778  0.000521  0.000716   \n",
       "2         position  0.000192  0.000236  0.000195  0.000181  0.000157   \n",
       "3           happen  0.000885  0.001267  0.000740  0.000634  0.001005   \n",
       "4               im  0.016197  0.015699  0.013981  0.012448  0.014812   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "52163        rikki  0.000000  0.000000  0.000001  0.000000  0.000000   \n",
       "52164     conveyer  0.000000  0.000000  0.000001  0.000000  0.000000   \n",
       "52165         bebo  0.000000  0.000000  0.000000  0.000000  0.000001   \n",
       "52166  frugalistas  0.000000  0.000000  0.000001  0.000000  0.000000   \n",
       "52167      tooheys  0.000000  0.000000  0.000001  0.000000  0.000000   \n",
       "\n",
       "       surprise         total  \n",
       "0      0.107420  1.084063e-01  \n",
       "1      0.000557  7.102294e-04  \n",
       "2      0.000142  1.851802e-04  \n",
       "3      0.001427  9.099117e-04  \n",
       "4      0.012466  1.451954e-02  \n",
       "...         ...           ...  \n",
       "52163  0.000000  3.718478e-07  \n",
       "52164  0.000000  3.718478e-07  \n",
       "52165  0.000000  3.718478e-07  \n",
       "52166  0.000000  3.718478e-07  \n",
       "52167  0.000000  3.718478e-07  \n",
       "\n",
       "[52168 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDEm(row):\n",
    "    results = []\n",
    "    for e in emotions:\n",
    "        results.append((row[0],e,row[1]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDEmWord(row):\n",
    "    results = []\n",
    "    for w in row[-1]:\n",
    "        results.append(((row[0],row[1]),list(words_prob.loc[words_prob.word == w][row[1]])[0]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEm(a,b):\n",
    "    if a[1]>=b[1]:\n",
    "        return a\n",
    "    else:\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictEmotion(rdd):\n",
    "    id_em = rdd.flatMap(getIDEm).flatMap(getIDEmWord).reduceByKey(\n",
    "        lambda a,b : a*b).map(\n",
    "        lambda row: (row[0][0],(row[0][1],row[1]*emotions_prob[row[0][1]]))).reduceByKey(getEm).map(\n",
    "        lambda row: {'id':row[0],'prediction':row[1][0]})\n",
    "    return id_em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice:\n",
    "## Executing the following cell will take almost an entire day (Took me about 7-8 hours). I have pickled the results into file \"HW2_P04_MahtabNejti_98209434_Pickled_Predict\". Skip this cell and load the results from the next cell.\n",
    "## If you want to run the algorithm, make sure you reduce the dataset size by limiting the baskets list.\n",
    "## I could have dropped a set of words with too little standard deviation in their distribution among feelings or used other dimenseionality reduction techniques to reduce the execution time. But I didn't have enough time to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready = sc.parallelize(baskets)\n",
    "# predictions = predictEmotion(ready).collect()\n",
    "# with open('HW2_P04_MahtabNejti_98209434_Pickled_Predict','wb') as f:\n",
    "#     pickle.dump(predictions,f)\n",
    "# predictions_df = sqlc.read.json(sc.parallelize(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HW2_P04_MahtabNejti_98209434_Pickled_Predict','rb') as f:\n",
    "    predictions = pickle.load(f)\n",
    "predictions_df = sqlc.read.json(sc.parallelize(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.alias('data')\n",
    "pred = predictions_df.alias('pred')\n",
    "\n",
    "final = data.join(pred,data.id == pred.id).select(data['*'],pred['prediction']).toPandas()\n",
    "final.to_csv(\"HW2_P04_MahtabNejti_98209434.csv\", header=True,index=False)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lost some rows! Why?\n",
    "### As shown below, amotion of sentences with too short and incignificant words (i.e. non-informative sentences) was not predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "successful = data.join(pred,data.id == pred.id).select(data['*'])\n",
    "lost = data.exceptAll(successful).toPandas()\n",
    "lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Precision, Recall, and F1-Socre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(final['emotion'])\n",
    "y = list(final['prediction'])\n",
    "print(sklearn.metrics.classification_report(x, y, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
